# Introduction {#introduction}

Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. It is becoming increasingly difficult for actuaries and other insurance professionals to obtain accurate onleveled premium at the exposure level.  This may happen for several reasons:

(1) The company did not invest in re-rating technologies, capturing critical information, or detailed exposure data.  
(2) The demands on the actuary to provide policy or exposure level data is increasing.
(3) Rating systems are on outdated IT platforms, many from the innovations in the 1970.  These systems work well but are not well understood in many cases.
(4) Business requirements were not robust enough or missed details when developing rating systems.

There is a need for a faster more accurate approach to approximate onlevel premiums.  Traditional approaches are time-consuming and are often left with problems that never get resolved^[The traditional approach when designing an extension of exposures algorithm is to code every rating step.  A batch rater would require data preprocessing as well--- this is often difficult if the data is not mapped to rating elements directly.].

(1) Bring policy premiums to current rate level for actuarial and other financial analysis.
(2) Verifying policy rating systems are compliant with intended rates.
(3) Data scienstists and research professionals need onleveled premiums for some models and the traditional techniques (such as pallelogram method) do not onlevel premium accurately enough at a lower grain.  Incorrect premiums can add skew and mis-align results. (Chuck, can you show this somehow?)
(4) Product/portfolio managers want to view their book of business in finer more segmented detail.  Accurate premiums can eliminate some bias or reduce the number of assumptions when facing a business decision.
(5) A rating system is often used to simulate rate changes and view impacts to policyholders.  These impacts, sometimes called dislocation, is often shared with US State Departments in the form of a histogram when a company is making pricing changes.

Over the years a company may have licensed many writing companies each with their "code" or rating algorithm across multiple systems with varying versions of code. This makes it difficult even more difficult to rate policies in batch. If IT and actuaries do not have the ability to do this then assumptions need to be made and particular details need to be addressed directly.

## Extension of Exposures 

Extension of exposures (EoE) is the method in which every policy is rated according to some "rate book" version which typically includes these components:

(1) Set of rate tables.
(2) Rating steps / Rating formula / Rating Algorithm.
(3) Policy attribute data / rating data.

This method restates the historical premium to the amount that would be charged under the rates set forth by the rate book.  In theory, this could be at any rate level as prescribed by the rate book version. Extension of exposures has the advantage of being the most accurate current rate level method, however, the major disadvantage it has is the amount of time and effort it takes to maintain.

In the past, extension of exposures was practically impossible due to the significant number of calculations required to rerate each policy. Given the
tremendous increase in computing power, the only remaining hurdle is associated with gathering the required data. To adjust premium to the current rate level using the extension of exposures technique, the practitioner needs to know the applicable rating characteristics for every policy in the historical period. Often companies do not have that information readily available.

The `leennr` approach is meant to address the disadvantages of the extension of exposures method: rating algorithm maintenance and data availability. Because rating algorithms are so unique there is no single approach that will work in all cases.  With luck this book's guidance will produce faster more granular on-leveled premiums.

## Why neural nets?

Neural nets have been said to compute any function within certain approximation (http://neuralnetworksanddeeplearning.com/chap4.html).  There is a tradeoff between accuracy and computational time; the deeper complexity of the neural net leads to more accuracy but longer learning time.  These approximations are good enough for most actuarial applications as we'll see later.

Premiums are a function of $x$ features, a continuous value greater than 0 but certainly not large.  Premiums come from a closed system with many but only finite possible outcomes. Neural networks approximate functions with such properties.  Not all outcomes are not evenly distributed which is often referred to as mix of business.  This may pose challenges when learning small segments of data which is a limitation of any model. 

The traditional approach consists of tracking down the exact value of every feature and matching the levels exactly to the rate tables.  When a change is made to the algorithm, a rating variable, or table structure, this must be manually programmed again by actuaries. It can be difficult to keep up with. The `leennr` approach uses neural networks to speed up this process by eliminating data to rate table mappings.  Rather than matching values to rate tables, the values can be treated like features in a model.  Each feature may have some bearing on the final result which is premium. In theory, the parameters learned will be close to the rating factors in the rate tables, but ultimately, if the algorithm approximates premium well enough for the purposes of the problem that is all that matters.

We are going to use neural networks to approximate the "premium function".  Getting within a specified tolerance is our goal.  The downside to this approach, which we will explore later, is the complete reliance on data. A quoting engine has different data than a policy writing engine as an example. The premium data generated and collected needs to be understood by the practitionor above all else when using `leennr`. For example, by its definition if the premium data has errors the neural network would target the "incorrect premiums", and learn a system that contains those errors.
