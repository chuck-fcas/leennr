---
title: "Learning Extension of Exposures with Neural Networks in R"
author: "Charles Lindberg"
date: "April 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
```

## Abstract

**Motivation.** Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. Over the years a company may have licensed many writing companies each with their "code" or rating algorithm across multiple systems with varying versions of code. This makes it difficult to rate policies in batch. IT and actuaries do not have the ability to do this. 

Replatforming is becoming more and more of an issue.

**Method.** It is standard practice to onlevel policy premiums to current (or prior) rate levels and best practice to use some form of extension of exposures.  A theory is proposed when data is missing.  Variability should not be an issue.  Rating algorithms are a closed system so there should be very little variability or not at all.

**Results.** As long as premiums.  The error term can be used to assess risk.  Error in premiums can be directly compared to actuarial indications.



**Conclusions.** Using a neural network: A PMML can be created, we can use many data scources.  

How does it handle mutiple writing companies.  Once state versus another.  State nuances?

**Availability.** The data source if the most important aspect.  The data needs to be from the direct source of the data.  A downstream source will lose information.

**Keywords.** Neural Networks; Extension of Exposures; Parallelogram Method; Onleveling;


## Introduction

Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. It is becoming increasingly difficult for actuaries and other insurance professionals to get extremely accurate information at the exposure level.  This may happen for several reasons:

(1) The company did not invest in re-rating technologies, capturing critical information, or detailed exposure detail.  
(2) The demands on the actuary to provide policy or exposure level data is increasing in demand.
(3) Rating systems are on outdated IT platforms, many from the innovations in the 1970s, .  These systems work but are not well understood in many cases.
(4) Business requirements did not capture all the or budget changes,  Information may be lost if business.

There is a need for actuaries, IT professionals, and data scientist to solve the replatform problem at a low cost.  The cost incurred to companies is passed back to consumers unless a conscious investment is made.

Policies ne

(1) Bring policy premiums to current rate level for actuarial and other financial analysis.
(2) Verifying policy rating systems are compliant with intended rates.
(3) Data scienstists and research professionals use onleveleld.  Modeling can add skew and mis-align results. (Chuck, can you show this somehow?)
(4) Product/portfolio managers want
(5) A rating system is often used to simulate

Over the years a company may have licensed many writing companies each with their "code" or rating algorithm across multiple systems with varying versions of code. This makes it difficult to rate policies in batch. IT and actuaries do not have the ability to do this.

## Extension of Exposures 

Extension of exposures (EoE) is the method in which every policy is rated according to some "rate book" version which typically includes these components:

  (1) Set of rate tables.
  (2) Rating steps / Rating formula / Rating Algorithm.
  (3) Policy attribute data / rating data.

This method restates the historical premium to the amount that would be charged under the rates set forth by the rate book.  In theory, thois could be at any rate level as prescribed by the rate book version.  However, this is often not the case.  These system

However, this implies a dataset does exist see \ref[theory]


Extension of exposures has the advantage of being the most accurate current rate level method, assuming the actuary has access to the detailed data required. 

In the past, extension of exposures was practically impossible due to the significant number of calculations required to rerate each policy. Given the
tremendous increase in computing power, the only remaining hurdle is associated with gathering the required data. To adjust premium to the current rate level using the extension of exposures technique, the practitioner needs to know the applicable rating characteristics for every policy in the historical period.
Often companies do not have that information readily available.

Returning to the example, assume the actuary wishes to adjust the historical premium for Policy
Year 2011 to the current rate level. Assume one such policy was effective on March 1, 2011 and had 10
class Y exposures. The actual premium charged for the policy was based on the rates effective on
January 1, 2011, and was $7,370 (= 10 x $1,045 x 0.60 + $1,100). To put the premium on-level,
substitute the current base rate, class factor, and policy fee in the calculations; this results in an on-level
premium of $8,405 (= 10 x $1,045 x 0.70 + $1,090). This same calculation is performed for every policy
written in 2011 and then aggregated across all policies.
If a group of policies has the exact same rating characteristics, they can be grouped for the purposes of the
extension of exposures technique. This type of grouping is—practically speaking—only relevant in lines
with relatively simple rating algorithms and very few rating variables.
In some commercial lines products, underwriters can apply subjective debits and credits to manual
premium. This complicates the use of the extension of exposures technique since it may be difficult to
determine what debits and credits would be applied under today’s schedule rating guidelines. The actuary
may consider measuring how credit and debit practices have changed by reviewing distributions of debits
and credits over recent years. 

## Why neural nets?

Neural nets have been said to compute any function within certain approximation (http://neuralnetworksanddeeplearning.com/chap4.html).  There is a tradeoff between accuracy and computational time; the deeper complexity of the neural net leads to more accuracy but longer learning time.  These approximations are good enough for actuaries (see later section). 

Premiums are a function of $x$ features, a closed system with many but only finit possible outcomes. Outcomes are not evenly distributed which is often referred to as mix of business.

Unstructured data can be used.

The traditional approach versus the NN approach.  The traditional approach consists of tracking down the exact value of every feature and matching the levels exactly to the.  When a change is made to the algorithm, a rating variable, or table structure, this must be manually programmed again by actuaries.

The NN approach is to approximate the function.  We want to get within a $1.  The downside, which we will explore later, is the .  The traditional approach must happen in the first place in order for premium data to be generated and collected. Neural nets rely on a sample of premium data.  By its definition if the premium data has errors the neural net would target the "incorrect premiums", and fit to a system that was not intended by the traditional approach.

## Hypothesis and Theory {#theory}

### Theory on the data.  

Logic: if policy premium is generated.

Show diagram of data in / data out.  

Data needs to be on one row for each exposure - that means we can test out the flat fee versus not flat fee.

### Theory on the rating algorithm

Neural nets should

## Rate Books and Rate Calculation Examples

In this section we provide rating examples we can reference.  

### Homeowners

Homeowners insurance covers damage to the property, contents, and outstanding structures, as well as loss of use, liability and medical coverage. The perils covered and amount of insurance provided is detailed in the policy contract.  

This rate book is based on all-perils combined and all coverages combined for simplicity.

#### Homeowners - Rating Algorithm

The rating algorithm is defined as follows.

```{r rating_algorithm, echo = FALSE, warning = FALSE}
tbl_algorithm <-
  tribble(
    ~variable, ~operation, ~rate_table,
    "All-Peril Base Rate", "+", "tbl_base",
    "AOI Relativity", "x", "tbl_aoi",
    "Territory Relativity", "x", "tbl_territory",
    "Protection Class / Construction Type Relativity", "x", "tbl_protection_class",
    "Underwriting Tier Relativity", "x", "tbl_uw_tier",
    "Deductible Credit", "x", "tbl_deductible",
    "New Home Discount", "x [1 - .]", "tbl_credits",
    "Claims-Free Discount", "x [1 - .]", "tbl_credits",
    "Multi-Policy Discount", "x [1 - .]", "tbl_credits",
    "Increased Jewelry Coverage Rate", "+", "tbl_jewelry",
    "Increased Liability/Medical Coverage Rate", "+", "tbl_increased_liability",
    "Policy Fee", "+", "tbl_expense_fee"
  ) 

tbl_algorithm %>% 
  transmute(
    expression = variable %>% paste0(" (", ., ")"),
    expression = 
      if_else(
        operation %>% str_detect("\\."), 
        operation %>% str_replace("\\.", expression), 
        map2_chr(operation, expression, function(x,y) paste0(x, y, collapse = " "))
  ))
```

The rate tables associated with each rating step can be mapped as follows.

```{r rating_algorithm_tabular, echo = FALSE}
tbl_algorithm %>% 
  select(-operation) %>% 
  knitr::kable(format = "html") %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

#### Homeowners - Rate Tables

The rate tables are defined as follows.  Note the amou

```{r ho_rate_tables, echo = FALSE}
ho_rate_tables <- 
  list(
    # Base rate
    tbl_base =
      tribble(
        ~peril,     ~value,
        "all_perils", 500),
    tbl_aoi =
      tribble(
        ~aoi,     ~value,
         80 ,0.56
        ,95 ,0.63
        ,110,0.69
        ,125,0.75
        ,140,0.81
        ,155,0.86
        ,170,0.91
        ,185,0.96
        ,200,1.00
        ,215,1.04
        ,230,1.08
        ,245,1.12
        ,260,1.16
        ,275,1.20
        ,290,1.24
        ,305,1.28
        ,320,1.32
        ,335,1.36
        ,350,1.39
        ,365,1.42
        ,380,1.45
        ,395,1.48
        ,410,1.51
        ,425,1.54
        ,440,1.57
        ,455,1.60
        ,470,1.63
        ,485,1.66
        ,500,1.69
        # ,15 ,0.03   Can't do each additional right now.
        ),
    tbl_territory =
      tribble(
        ~territory,     ~value,
        1,0.80,
        2,0.90,
        3,1.00,
        4,1.10,
        5,1.15),
    tbl_protection_class = 
      tribble(
        ~protection_class, ~construction_type, ~value,
        1, "frame",  1.00, 
        2, "frame",  1.00, 
        3, "frame",  1.00, 
        4, "frame",  1.00, 
        5, "frame",  1.05, 
        6, "frame",  1.10, 
        7, "frame",  1.15, 
        8, "frame",  1.25, 
        9, "frame",  2.10, 
        10,"frame",  2.30, 
        1, "masonry",0.90,
        2, "masonry",0.90,
        3, "masonry",0.90,
        4, "masonry",0.90,
        5, "masonry",1.00,
        6, "masonry",1.05,
        7, "masonry",1.10,
        8, "masonry",1.15,
        9, "masonry",1.75,
        10,"masonry",1.90),
    tbl_uw_tier =
      tibble(
        uw_tier = LETTERS[1:10],
        value    = 1:10 * 0.12 + 0.7),
    tbl_deductible =
      tribble(
        ~deductible, ~value,
        250,  1.00,
        500,  0.95,
        1000, 0.85,
        5000, 0.70),
    tbl_credits =
      tribble(
        ~credit,       ~value,
        "new home",    0.2,
        "claims free", 0.1,
        "multipolicy", 0.08),
    tbl_jewelry =
      tribble(
        ~jewelry_limit, ~value,
        2500, 0,
        5000, 35,
        10000, 60),
    tbl_increased_liability =
      tribble(
        ~limit_liability_medpay, ~value,
        "$100,000/$500",   0,
        "$300,000/$1,000", 25,
        "$500,000/$2,500", 45),
    tbl_expense_fee =
      tribble(
        ~form, ~value,
        "HO3", 50)
  )

ho_rate_tables 
```

#### Homeowners - Manual Rate Calculation

WGIC is preparing a renewal quote for a homeowner currently insured with Wicked Good. The policy
has the following risk characteristics:
  - Amount of insurance = $215,000
  - The insured lives in Territory 4.
  - The home is frame construction located in Fire Protection Class 7.
  - Based on the insured’s credit score, tenure with the company, and prior loss history, the
policy has been placed in Underwriting Tier C.
  - The insured opts for a $1,000 deductible.
  - The home falls under the definition of a new home as defined in Wicked Good’s rating rules.
  - The insured is eligible for the five-year claims-free discount.
  - There is no corresponding auto or excess liability policy written with WGIC.
  - The policyholder opts to increase coverage for jewelry to $5,000 and to increase
liability/medical coverage limits to $300,000/$1,000. 

### Rate Book Complexity

One way to measure the complexity of a rate book is by how many price points are possible.  In practice, this doesn't always mean every scenario will occur or be possible.  For example, age of insured is a widely used rating variable with high predictive power; the rate table may contain values for 120 years olds, but in reality there are no 120 year olds who need insurance.  That being said, the number of possible price points may also measure flexibility of the rating system.  Complexity in this case is referring only to the number of possibilities, not a measure of sophistication.

The number of possible price points is as follows:

```{r}
tribble(
  ~rate_book, ~complexity, 
  "Homeowners", ho_rate_tables %>% sapply(nrow) %>% prod
  )
```

## Generating Premium Data {#premium_data}

To get get started we need data that contains rating characteristics. These rating characteristics (also called features) will be passed through a rate book to generate premiums.  In this section, we create datasets using the rating examples we saw in the last section.  Effectively, we can generate a book of business with any particular mix we like. This will come in handy later when test assumptions and prove the merits of this approach.




### Uniformly Distributed, Independent Mix, with Exact Features

Let's define what is meant by uniformly distributed, independent mix, with exact features.

  - *Uniformly Distributed*: each level has the same chance of being drawn as any other level.
  - *Independent Mix*: rating variables are not correlated with one another whatsoever.
  - *Exact Features*: features are drawn from the rate tables so the exact value can be found in the rate table.

This is the simplest dataset to create.  We randomly sample a row from each rate table and bind the result all together to create the exposure record. This code generates feature data we can use to rate with.  

```{r uniform_dist_iid}
# Number of records we want to generate.
n_draw <- 10000

# For each table, draw n_draw random numbers.
uniform_dist_iid <-
  ho_rate_tables %>% 
  lapply(
    function(x) {
      nrow(x) %>% 
      sample(n_draw, replace = TRUE) %>% 
      lapply(function(y) x[y, -ncol(x)]) %>% bind_rows}) %>% 
  bind_cols %>% 
  cbind(policy_number = 1:nrow(.) %>% str_pad(nchar(n_draw), pad = "0"), .)

uniform_dist_iid %>% as_tibble
```

Next, retrieve the rating factors and apply them according to the rating algorithm steps to generate premium as follows.  One thing to mention here.  In reality a policy can have multiple credits but this code only randomly draws one from the credit table the way it is setup.  This doesn't take away from our experiment.

```{r}
# Need to use the formula we already laid out above somehow.  This is just to keep going.
uniform_dist_iid_rating <-
  ho_rate_tables %>% 
    lapply(function(x) (x %>% right_join(uniform_dist_iid))$value) %>% 
  bind_rows %>% 
  mutate(
    premium =
      tbl_base *
      tbl_aoi *
      tbl_territory * 
      tbl_protection_class *
      tbl_uw_tier *
      tbl_deductible *
      (1 - tbl_credits) +
      tbl_jewelry +
      tbl_increased_liability +
      tbl_expense_fee,
    premium = premium %>% round
  )

# Inpute premium data onto rating data
uniform_dist_iid <- 
  uniform_dist_iid %>% 
  cbind(premium = uniform_dist_iid_rating$premium) %>% 
  mutate_if(is.character, factor)
```

Let's take a look at the distribution of premiums.

```{r fig.align = 'center'}
uniform_dist_iid %>% 
  ggplot + 
  geom_bar(aes(x = premium))
```

This is great result. The ratings factors themselves are generated using loss cost models.  Even though we randomly generated a feature set there is a clear transfomation that occurs just by using rating factors selected from generlized linear model (usually).

### Not Uniformly Distributed, Independent Mix of Business

### Not Uniformly Distributed, Dependent Mix of Business

### Generating Historical Book of Business

To this point we have generated datasets that mirror what we'll call a quoting environment rather than a policy writing environment. A quoting environment is only generating full-term premiums based on the characteristics provided.  The policy writing system includes endorsements which can generate .  The main reason for the differences is simple: time. Quoting is at one point in time with rating features fixed, but a policy can change during it's term.  In other words, the rating features can change over the life of the policy contract.

In this section we develop a mock historical book of business.  To keep it simple, we are not going to use new business and retention assumptions.  In other words, we will start with one set of simulated policy data and age it to simulate five years worth of premium data.  Policies falling off the book and new business coming on will shift the mix but for it isn't needed for now.  In addition, we are not going to 

#### Uniformly Distributed

```{r}
uniform_dist_iid$policy_year <- 
  Sys.Date() %>% 
  lubridate::year() - 5

bob_uniform_dist_iid <-
  uniform_dist_iid %>% 
  mutate(
    policy_year = policy_year + 1,
    aoi         = (1.05 * aoi) %>% round
    )
  lubridate::ceiling_date(unit = "years") - 1
  lubridate::floor_date(unit = "years")
  lubridate::year()

# An example of sampling dates
# sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), nrow(uniform_dist_iid), replace = TRUE)
```

#### Rate Book Changes



### Extreme Sample Bias

What if 90% of the data was male rather than female.

## Learning Extension of Exposures - Proof of Concept

### Data Preparation and Required Packages

Data has to be on some exposure basis with each record (row) containing the premium target.  Each row is a training example.  See premium_data\@ref[premium_data] for more detail. Data must also be hot key encoded before processing as well.

The `keras` package offers a great way to quickly fit neural nets for research. Refer to the `keras` package for more detail and most everything you need to know about modeling with `keras`. The [Keras site](https://keras.rstudio.com/) is a great place to start.  I'm using the GPU setup of tensorflow--- it takes longer to setup but it fits the model much faster.

```{r echo = FALSE}
library(keras)
# keras::install_keras(tensorflow = "gpu")

set.seed(5555)
index_sample <- sample(0.8 * nrow(uniform_dist_iid))

x_train <- uniform_dist_iid[ index_sample,] %>% select(-premium, -policy_number, -peril)
y_train <- uniform_dist_iid[ index_sample,] %>% select( premium) %>% as.matrix
x_test  <- uniform_dist_iid[-index_sample,] %>% select(-premium, -policy_number, -peril)
y_test  <- uniform_dist_iid[-index_sample,] %>% select( premium) %>% as.matrix

x_train <- 
  x_train %>% 
  mutate_if(is.factor, as.numeric) %>% 
  mutate_if(is.factor, to_categorical) %>% 
  bind_cols %>% 
  as.matrix

x_test <- 
  x_test %>% 
  mutate_if(is.factor, as.numeric) %>% 
  mutate_if(is.factor, to_categorical) %>% 
  bind_cols %>% 
  as.matrix
```

Note we did not scale any of the features or the output.  This is not best practice.  Because we're not using many features, the dataset is pretty small (e.g. `r n_draw` rows) and we know the range of premiums is limited from `r min(uniform_dist_iid$premium)` to `r max(uniform_dist_iid$premium)` it won't take extra computational time.

### Model Setup

We keep the model pretty simple for now.  The ReLu activation is used to keep things greater than zero and not limited by a maximum.
 
```{r}
model <- 
  keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'linear', input_shape = c(x_train %>% ncol)) %>% 
  layer_dense(units = 32, activation = 'linear') %>%
  layer_dense(units = 1 , activation = 'relu')

model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_rmsprop()
)
```
### First Time leennR

Fitting the model is quite simple. The model is trained for 300 epochs.

```{r first_time_leennr_fit, echo = FALSE}
history <- 
  model %>% 
  fit(
  x_train,
  y_train, 
  epochs = 300, 
  batch_size = 128, 
  validation_split = 0.2,
  )
```

The model summary, the fitting history, and the mean squared error are as follows. 

```{r fig.align = 'center'}
model %>% summary()

plot(history)

mse <- history$metrics$loss %>% tail(., 1) / nrow(x_train) 

paste0("Mean absolute error on test set: $", sprintf("%.2f", mse))

model %>% evaluate(x_test, y_test)
```

We can plot how far off the predictions are with histograms.  The results show we have some additional research to perform if we want to refine the accuracy further.

```{r}
predict <-
  x_train %>% 
  cbind.data.frame(
    y_train, 
    y_hat = 
      model %>% 
      predict(x_train)) %>% 
  as_tibble

predict_test <-
  x_test %>% 
  cbind.data.frame(
    y_train, 
    y_hat = 
      model %>% 
      predict(x_test)) %>% 
  as_tibble

# predict %>%  
#   ggplot + 
#   geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) +
#   xlab("Premium") +
#   ylab("Absolute Error as % of Premium")

predict %>%  
  ggplot + 
  geom_hex(aes(x = premium, y = premium - y_hat)) +
  xlab("Premium") +
  ylab("Premium - Fitted Value") 
# predict_test %>%  
#   ggplot + 
#   geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) +
#   xlab("Premium") +
#   ylab("Absolute Error as % of Premium")

predict_test %>%  
  ggplot + 
  geom_hex(aes(x = premium, y = premium - y_hat)) +
  xlab("Premium") +
  ylab("Premium - Fitted Value") 
```

```{r}
uniform_dist_iid %>% 
  ggplot + 
  geom_histogram(aes(x = premium))
```

### Randomness and error of fit

Over fitting?  We should overfit!  Error is missing data or incorrect structure of net (need deeper or mimic rating algorithm).

Loss data is random.  Premiums should not be.  Use the GLM argument.  Use the closed system argument.

Paralellogram method versus leennR error.  This will be tested in the next section.  

The P method assumes the following: 

## The leennr Proof of Concept

The goal is to reasonably approximate rate changes for the 

### Simulate Rate Changes - this is needed to 




### Simulate Written Premium Transactions


Does it matter if we use earnings or written premiuums


## Research

Further research needs to be done in order to justify this work with the department of insurance.  In other words, what guidance can be provided so that departments of insurance can accept this method.

Here we resarch possible pitfalls with this method and the limitations.  Also, can we use these models for production purposes.  

### Number of Price Points and Needed Layers



### Testing Missing Features



### Is it right?

Depends on the data.  If the insurance data is wrong it produce correct onleveled premiums for the actuary but it will not point out production problems in rating.  

A few rating examples can datermine.


### Method

How am I approaching data creation.

#### Does it matter if we do not correlation among cohorts?

##### Without correlation

##### With correlation

## More

### Rating Expert Feedback

Feedback is very important to any model.  Individual rating examples provided by actuarial or other staff 

### Revisiting Rating Examples with Deeper Nets

With all the background up to this point we introduce depper networks to represenet non-linear rating example.
