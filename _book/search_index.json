[
["index.html", "Learning Extension of Exposures with Neural Networks in R Abstract", " Learning Extension of Exposures with Neural Networks in R Charles Lindberg, FCAS 2019-06-07 Abstract Motivation. Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. Over the years a company may have licensed many writing companies each with their “code” or rating algorithm across multiple systems with varying versions of code. This makes it difficult to rate policies in batch. IT and actuaries do not have the ability to do this. Replatforming is becoming more and more of an issue. Method. It is standard practice to onlevel policy premiums to current (or possibly prior) rate levels and it is best practice to use some adaptation of extension of exposures. A theory is proposed when data is missing. Variability should not be an issue- rating algorithms are a closed system so there should be very little variability or not at all. Results. As long as premiums are rated correctly. The error term can be used to assess risk. Error in premiums can be directly compared to actuarial indications. Conclusions. Using a neural network: A PMML can be created, we can use many data scources. How does it handle mutiple writing companies. Once state versus another. State nuances? Availability. The data source if the most important aspect. The data needs to be from the direct source of the data. A downstream source will lose information. Keywords. Neural Networks; Extension of Exposures; Parallelogram Method; Onleveling; "],
["about-the-author.html", "About the Author", " About the Author Chuck Lindberg is a Fellow of the Casualty Actuarial Society who studied mathematics at Utah Valley University. He has ten years of experience in US Property &amp; Casualty insurance with applications in predictice modeling and data science. Lindberg is a strong advocate for modern actuarial best practices to include reproducibility and make it simpler for actuaries to research, analyze, and interpret rather than crunch figures. "],
["introduction.html", "Section 1 Introduction", " Section 1 Introduction Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. It is becoming increasingly difficult for actuaries and other insurance professionals to obtain accurate onleveled premium at the exposure level. This may happen for several reasons: The company did not invest in re-rating technologies, capturing critical information, or detailed exposure data. The demands on the actuary to provide policy or exposure level data is increasing. Rating systems are on outdated IT platforms, many from the innovations in the 1970. These systems work well but are not well understood in many cases. Business requirements were not robust enough or missed details whenb developing rating systems. There ia a need for a faster more accurate approach to approximate onlevel premiums. Traditional approaches are time-comsuming and are often left with problems that never get resolved. An approach that is agnostic to Bring policy premiums to current rate level for actuarial and other financial analysis. Verifying policy rating systems are compliant with intended rates. Data scienstists and research professionals need onleveled premiums for some models and the traditional techniques (such as pallelogram method) do not onelevel premium accurately enough at a lower grain. Incorrect premiums can add skew and mis-align results. (Chuck, can you show this somehow?) Product/portfolio managers want to view there book of business at a finer A rating system is often used to simulate. Over the years a company may have licensed many writing companies each with their “code” or rating algorithm across multiple systems with varying versions of code. This makes it difficult to rate policies in batch. If IT and actuaries do not have the ability to do this then assumptions to be made and particular details need to be addressed directly. "],
["extension-of-exposures.html", "1.1 Extension of Exposures", " 1.1 Extension of Exposures Extension of exposures (EoE) is the method in which every policy is rated according to some “rate book” version which typically includes these components: Set of rate tables. Rating steps / Rating formula / Rating Algorithm. Policy attribute data / rating data. This method restates the historical premium to the amount that would be charged under the rates set forth by the rate book. In theory, thois could be at any rate level as prescribed by the rate book version. However, this is often not the case. These system However, this implies a dataset does exist see 1.2.1. Extension of exposures has the advantage of being the most accurate current rate level method, assuming the actuary has access to the detailed data required. In the past, extension of exposures was practically impossible due to the significant number of calculations required to rerate each policy. Given the tremendous increase in computing power, the only remaining hurdle is associated with gathering the required data. To adjust premium to the current rate level using the extension of exposures technique, the practitioner needs to know the applicable rating characteristics for every policy in the historical period. Often companies do not have that information readily available. Returning to the example, assume the actuary wishes to adjust the historical premium for Policy Year 2011 to the current rate level. Assume one such policy was effective on March 1, 2011 and had 10 class Y exposures. The actual premium charged for the policy was based on the rates effective on January 1, 2011, and was $7,370 (= 10 x $1,045 x 0.60 + $1,100). To put the premium on-level, substitute the current base rate, class factor, and policy fee in the calculations; this results in an on-level premium of $8,405 (= 10 x $1,045 x 0.70 + $1,090). This same calculation is performed for every policy written in 2011 and then aggregated across all policies. If a group of policies has the exact same rating characteristics, they can be grouped for the purposes of the extension of exposures technique. This type of grouping is—practically speaking—only relevant in lines with relatively simple rating algorithms and very few rating variables. In some commercial lines products, underwriters can apply subjective debits and credits to manual premium. This complicates the use of the extension of exposures technique since it may be difficult to determine what debits and credits would be applied under today’s schedule rating guidelines. The actuary may consider measuring how credit and debit practices have changed by reviewing distributions of debits and credits over recent years. "],
["why-neural-nets.html", "1.2 Why neural nets?", " 1.2 Why neural nets? Neural nets have been said to compute any function within certain approximation (http://neuralnetworksanddeeplearning.com/chap4.html). There is a tradeoff between accuracy and computational time; the deeper complexity of the neural net leads to more accuracy but longer learning time. These approximations are good enough for actuaries (see later section). Premiums are a function of \\(x\\) features, a closed system with many but only finit possible outcomes. Outcomes are not evenly distributed which is often referred to as mix of business. Unstructured data can be used. The traditional approach versus the NN approach. The traditional approach consists of tracking down the exact value of every feature and matching the levels exactly to the. When a change is made to the algorithm, a rating variable, or table structure, this must be manually programmed again by actuaries. The NN approach is to approximate the function. We want to get within a $1. The downside, which we will explore later, is the . The traditional approach must happen in the first place in order for premium data to be generated and collected. Neural nets rely on a sample of premium data. By its definition if the premium data has errors the neural net would target the “incorrect premiums”, and fit to a system that was not intended by the traditional approach. 1.2.1 Hypothesis and Theory 1.2.1.1 Theory on the data. Logic: if policy premium is generated. Show diagram of data in / data out. Data needs to be on one row for each exposure - that means we can test out the flat fee versus not flat fee. 1.2.1.2 Theory on the rating algorithm Neural nets should "],
["ratebooks.html", "Section 2 Rate Books and Rate Calculation Examples", " Section 2 Rate Books and Rate Calculation Examples In this section we provide rating examples we can reference. "],
["homeowners.html", "2.1 Homeowners", " 2.1 Homeowners Homeowners insurance covers damage to the property, contents, and outstanding structures, as well as loss of use, liability and medical coverage. The perils covered and amount of insurance provided is detailed in the policy contract. This rate book is based on all-perils combined and all coverages combined for simplicity. 2.1.1 Homeowners - Rating Algorithm The rating algorithm is defined as follows. ## # A tibble: 12 x 1 ## expression ## &lt;chr&gt; ## 1 + (All-Peril Base Rate) ## 2 x (AOI Relativity) ## 3 x (Territory Relativity) ## 4 x (Protection Class / Construction Type Relativity) ## 5 x (Underwriting Tier Relativity) ## 6 x (Deductible Credit) ## 7 x [1 - (New Home Discount)] ## 8 x [1 - (Claims-Free Discount)] ## 9 x [1 - (Multi-Policy Discount)] ## 10 + (Increased Jewelry Coverage Rate) ## 11 + (Increased Liability/Medical Coverage Rate) ## 12 + (Policy Fee) The rate tables associated with each rating step can be mapped as follows. variable rate_table All-Peril Base Rate tbl_base AOI Relativity tbl_aoi Territory Relativity tbl_territory Protection Class / Construction Type Relativity tbl_protection_class Underwriting Tier Relativity tbl_uw_tier Deductible Credit tbl_deductible New Home Discount tbl_credits Claims-Free Discount tbl_credits Multi-Policy Discount tbl_credits Increased Jewelry Coverage Rate tbl_jewelry Increased Liability/Medical Coverage Rate tbl_increased_liability Policy Fee tbl_expense_fee 2.1.2 Homeowners - Rate Tables The rate tables are defined as follows. Note the amount of insurance (aoi) table does not typically have a max. Programming each additional factors (extrapolation) and interpolation can be cumbersome. At this time, we use rounded aoi and use a direct lookup rather than interpolation which is often used in practice. peril value all_perils 500 , aoi value 80 0.56 95 0.63 110 0.69 125 0.75 140 0.81 155 0.86 170 0.91 185 0.96 200 1.00 215 1.04 230 1.08 245 1.12 260 1.16 275 1.20 290 1.24 305 1.28 320 1.32 335 1.36 350 1.39 365 1.42 380 1.45 395 1.48 410 1.51 425 1.54 440 1.57 455 1.60 470 1.63 485 1.66 500 1.69 , territory value 1 0.80 2 0.90 3 1.00 4 1.10 5 1.15 , protection_class construction_type value 1 frame 1.00 2 frame 1.00 3 frame 1.00 4 frame 1.00 5 frame 1.05 6 frame 1.10 7 frame 1.15 8 frame 1.25 9 frame 2.10 10 frame 2.30 1 masonry 0.90 2 masonry 0.90 3 masonry 0.90 4 masonry 0.90 5 masonry 1.00 6 masonry 1.05 7 masonry 1.10 8 masonry 1.15 9 masonry 1.75 10 masonry 1.90 , uw_tier value A 0.82 B 0.94 C 1.06 D 1.18 E 1.30 F 1.42 G 1.54 H 1.66 I 1.78 J 1.90 , deductible value 250 1.00 500 0.95 1000 0.85 5000 0.70 , credit value new home 0.20 claims free 0.10 multipolicy 0.08 , jewelry_limit value 2500 0 5000 35 10000 60 , limit_liability_medpay value $100,000/$500 0 $300,000/$1,000 25 $500,000/$2,500 45 , form value HO3 50 "],
["rating-calculation-examples.html", "2.2 Rating Calculation Examples", " 2.2 Rating Calculation Examples In this section we provide a rating example for each rate book presented as a full-term quoted premium. 2.2.1 Homeowners The policy has the following risk characteristics: Amount of insurance = $275,000. The insured lives in Territory 2. The home is frame construction located in Fire Protection Class 8. Based on the insured’s credit score, tenure with the company, and prior loss history, the policy has been placed in Underwriting Tier C. The insured opts for a $1,000 deductible. The home falls under the definition of a new home as defined by the rating rules. The insured is eligible for the five-year claims-free discount. There is no corresponding auto or excess liability policy. The policyholder opts to increase coverage for jewelry to $5,000 and to increase liability/medical coverage limits to $300,000/$1,000. The calculation is expressed as follows. \\(500 \\times 1.20 \\times 0.9 \\times 1.25 \\times 1.06 \\times 0.85 \\times (1 - 0.2) \\times (1 - 0.1) \\times (1 - 0) + 35 + 50 = 522.9\\) The final result is rounded to the nearest dollar so the final premium for this quote is $523. "],
["rate-book-complexity.html", "2.3 Rate Book Complexity", " 2.3 Rate Book Complexity One way to measure the complexity of a rate book is by how many price points are possible. In practice, this doesn’t always mean every scenario will occur or be possible. For example, age of insured is a widely used rating variable with high predictive power; the rate table may contain values for 120 years olds, but in reality there are no 120 year olds who need insurance. That being said, the number of possible price points may also measure flexibility of the rating system. Complexity in this case is referring only to the number of possibilities, not a measure of sophistication. The number of possible price points for each rate book is as follows: tribble( ~rate_book, ~complexity, &quot;Homeowners&quot;, ho_rate_tables %&gt;% sapply(nrow) %&gt;% prod ) %&gt;% kableExtra::kable(format = &quot;html&quot;) %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) rate_book complexity Homeowners 3132000 "],
["premium-data.html", "Section 3 Generating Premium Data", " Section 3 Generating Premium Data To get get started we need data that contains rating characteristics. These rating characteristics (also called features) will be passed through a rate book to generate premiums. In this section, we create datasets using the rating examples we saw in the last section. Effectively, we can generate a book of business with any particular mix we like. This will come in handy later when test assumptions and prove the merits of this approach. "],
["uniformly-distributed-independent-mix-with-exact-features.html", "3.1 Uniformly Distributed, Independent Mix, with Exact Features", " 3.1 Uniformly Distributed, Independent Mix, with Exact Features Let’s define what is meant by uniformly distributed, independent mix, with exact features. Uniformly Distributed: each level has the same chance of being drawn as any other level. Independent Mix: rating variables are not correlated with one another whatsoever. Exact Features: features are drawn from the rate tables so the exact value can be found in the rate table. This is the simplest dataset to create. We randomly sample a row from each rate table and bind the result all together to create the exposure record. This code generates feature data we can use to rate with. # Number of records we want to generate. n_draw &lt;- 10000 ho_rate_tables &lt;- read_rds(&quot;ho_rate_tables.Rds&quot;) #Set seed to get consistent results each draw. set.seed(5555) # For each table, draw n_draw random numbers. ho_uniform_iid &lt;- ho_rate_tables %&gt;% lapply( function(x) { nrow(x) %&gt;% sample(n_draw, replace = TRUE) %&gt;% lapply(function(y) x[y, -ncol(x)]) %&gt;% bind_rows}) %&gt;% bind_cols %&gt;% cbind(policy_number = 1:nrow(.) %&gt;% str_pad(nchar(n_draw), pad = &quot;0&quot;), .) ho_uniform_iid %&gt;% as_tibble ## # A tibble: 10,000 x 12 ## policy_number peril aoi territory protection_class ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00001 all_~ 140 3 10 ## 2 00002 all_~ 125 1 7 ## 3 00003 all_~ 80 5 6 ## 4 00004 all_~ 290 4 3 ## 5 00005 all_~ 290 4 1 ## 6 00006 all_~ 170 3 4 ## 7 00007 all_~ 230 2 2 ## 8 00008 all_~ 290 1 8 ## 9 00009 all_~ 140 1 6 ## 10 00010 all_~ 95 3 9 ## # ... with 9,990 more rows, and 7 more variables: ## # construction_type &lt;chr&gt;, uw_tier &lt;chr&gt;, ## # deductible &lt;dbl&gt;, credit &lt;chr&gt;, ## # jewelry_limit &lt;dbl&gt;, limit_liability_medpay &lt;chr&gt;, ## # form &lt;chr&gt; Next, retrieve the rating factors and apply them according to the rating algorithm steps to generate premium as follows. One thing to mention here. In reality a policy can have multiple credits but this code only randomly draws one from the credit table the way it is setup. This doesn’t take away from our experiment. # Need to use the formula we already laid out above somehow. This is just to keep going. ho_uniform_iid_rating &lt;- ho_rate_tables %&gt;% lapply(function(x) (x %&gt;% right_join(ho_uniform_iid))$value) %&gt;% bind_rows %&gt;% mutate( premium = tbl_base * tbl_aoi * tbl_territory * tbl_protection_class * tbl_uw_tier * tbl_deductible * (1 - tbl_credits) + tbl_jewelry + tbl_increased_liability + tbl_expense_fee, premium = premium %&gt;% round ) ## Joining, by = &quot;peril&quot; ## Joining, by = &quot;aoi&quot; ## Joining, by = &quot;territory&quot; ## Joining, by = c(&quot;protection_class&quot;, &quot;construction_type&quot;) ## Joining, by = &quot;uw_tier&quot; ## Joining, by = &quot;deductible&quot; ## Joining, by = &quot;credit&quot; ## Joining, by = &quot;jewelry_limit&quot; ## Joining, by = &quot;limit_liability_medpay&quot; ## Joining, by = &quot;form&quot; # Inpute premium data onto rating data ho_uniform_iid &lt;- ho_uniform_iid %&gt;% cbind(premium = ho_uniform_iid_rating$premium) %&gt;% mutate_if(is.character, factor) Let’s take a look at the distribution of premiums. ho_uniform_iid %&gt;% ggplot + geom_histogram(aes(x = premium), binwidth = 50, color = &quot;darkorange3&quot;, fill = &quot;white&quot;) + xlab(&quot;Premium Bin&quot;) + ylab(&quot;Number of Training Examples&quot;) + theme_bw() This is great result. An insurance rating plan uses ratings factors typically generated using loss cost models. Even though we randomly generated a feature set there is a clear transfomation that occurs just by using rating factors selected from generlized linear model. "],
["not-uniformly-distributed-independent-mix-of-business.html", "3.2 Not Uniformly Distributed, Independent Mix of Business", " 3.2 Not Uniformly Distributed, Independent Mix of Business "],
["not-uniformly-distributed-dependent-mix-of-business.html", "3.3 Not Uniformly Distributed, Dependent Mix of Business", " 3.3 Not Uniformly Distributed, Dependent Mix of Business "],
["generating-historical-book-of-business.html", "3.4 Generating Historical Book of Business", " 3.4 Generating Historical Book of Business To this point we have generated datasets that mirror what we’ll call a quoting environment rather than a policy writing environment. A quoting environment is only generating full-term premiums based on the characteristics provided. The policy writing system includes endorsements which can generate different premiums over time if the endorsement is “premium bearing”. The main reason for the differences is simple: time. Quoting is at one point in time with rating features fixed, but a policy can change during it’s term. In other words, the rating features can change over the life of the policy contract for various reasons— for example, if the insured changes their policy deductible mid-term. In this section we develop a mock historical book of business. To keep it simple, we are not going to use new business and retention assumptions. In other words, we will start with one set of simulated policy data and age it to simulate five years worth of premium data with rate changes introduced. Policies falling off the book and new business coming on will shift the mix but the @refproof of concept does not require such detail at this time. In addition, again for simplicity, we are going to build in a rate change for each each year— so if we generate five years of data we will include four rate changes. It is important to note 3.4.1 Uniformly Distributed ho_uniform_iid$policy_year &lt;- Sys.Date() %&gt;% lubridate::year() - 5 bob_ho_uniform_iid &lt;- ho_uniform_iid %&gt;% mutate( policy_year = policy_year + 1, aoi = (1.05 * aoi) %&gt;% round ) # lubridate::ceiling_date(unit = &quot;years&quot;) - 1 # lubridate::floor_date(unit = &quot;years&quot;) # lubridate::year() # An example of sampling dates # sample(seq(as.Date(&#39;1999/01/01&#39;), as.Date(&#39;2000/01/01&#39;), by=&quot;day&quot;), nrow(ho_uniform_iid), replace = TRUE) ho_uniform_iid %&gt;% write_rds(&quot;ho_uniform_iid.Rds&quot;) 3.4.2 Rate Book Changes "],
["extreme-sample-bias.html", "3.5 Extreme Sample Bias", " 3.5 Extreme Sample Bias What if 90% of the data was male rather than female. "],
["first-time.html", "Section 4 Learning Extension of Exposures - Fitting a Dataset ", " Section 4 Learning Extension of Exposures - Fitting a Dataset "],
["data-preparation-and-required-packages.html", "4.1 Data Preparation and Required Packages", " 4.1 Data Preparation and Required Packages Data has to be on some exposure basis with each record (row) containing the premium target. Each row is a training example. See premium_data@ref[premium_data] for more detail. Data must also be hot key encoded before processing as well. The keras package offers a great way to quickly fit neural nets for research. Refer to the keras package for more detail and most everything you need to know about modeling with keras. The Keras site is a great place to start. I’m using the GPU setup of tensorflow— it takes longer to setup but it fits the model much faster. Note we did not scale any of the features or the output. This is not best practice. Because we’re not using many features, the dataset is pretty small (e.g. 10^{4} rows) and we know the range of premiums is limited from 170 to 3398 it won’t take extra computational time. "],
["model-setup.html", "4.2 Model Setup", " 4.2 Model Setup We keep the model pretty simple for now. The ReLu activation is used to keep things greater than zero and not limited by a maximum. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 64, activation = &#39;linear&#39;, input_shape = c(x_train %&gt;% ncol)) %&gt;% layer_dense(units = 32, activation = &#39;linear&#39;) %&gt;% layer_dense(units = 1 , activation = &#39;relu&#39;) model %&gt;% compile( loss = &#39;mean_absolute_error&#39;, optimizer = optimizer_rmsprop() ) "],
["first-time-leennr.html", "4.3 First Time leennR", " 4.3 First Time leennR Fitting the model is quite simple. This model will is trained for 300 epochs. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 300, batch_size = 128, validation_split = 0.2, ) The model summary, the fitting history, and the mean squared error are as follows. model %&gt;% summary() ## _______________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================= ## dense (Dense) (None, 64) 768 ## _______________________________________________________ ## dense_1 (Dense) (None, 32) 2080 ## _______________________________________________________ ## dense_2 (Dense) (None, 1) 33 ## ======================================================= ## Total params: 2,881 ## Trainable params: 2,881 ## Non-trainable params: 0 ## _______________________________________________________ plot(history) mse &lt;- history$metrics$loss %&gt;% tail(., 1) / nrow(x_train) paste0(&quot;Mean absolute error on test set: $&quot;, sprintf(&quot;%.2f&quot;, mse)) ## [1] &quot;Mean absolute error on test set: $0.02&quot; We can plot how far off the predictions are with histograms. The results show we have some additional research to perform if we want to refine the accuracy further. predict &lt;- x_train %&gt;% cbind.data.frame( y_train, y_hat = model %&gt;% predict(x_train)) %&gt;% as_tibble predict_test &lt;- x_test %&gt;% cbind.data.frame( y_train, y_hat = model %&gt;% predict(x_test)) %&gt;% as_tibble # predict %&gt;% # ggplot + # geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) + # xlab(&quot;Premium&quot;) + # ylab(&quot;Absolute Error as % of Premium&quot;) predict %&gt;% ggplot + geom_hex(aes(x = premium, y = premium - y_hat)) + xlab(&quot;Premium&quot;) + ylab(&quot;Premium - Fitted Value&quot;) # predict_test %&gt;% # ggplot + # geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) + # xlab(&quot;Premium&quot;) + # ylab(&quot;Absolute Error as % of Premium&quot;) predict_test %&gt;% ggplot + geom_hex(aes(x = premium, y = premium - y_hat)) + xlab(&quot;Premium&quot;) + ylab(&quot;Premium - Fitted Value&quot;) ho_uniform_iid %&gt;% ggplot + geom_histogram(aes(x = premium)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. "],
["poc.html", "Section 5 leennr Proof of Concept", " Section 5 leennr Proof of Concept There are two different parts to the leennr proof of concept. First, can fitting neural networks approximate extension of exposures within a reasonable tolerance and second, the more difficult can we use the neural nets as a valid rating algorithm, i.e. can we file it? In section @ref(first_time_fit) we showed how to fit a neural network to premium data. The next step is to expand on this idea and determine if the method does reasonably well modeling a rate structure to re-rate polices to a specific rate book (i.e. rate level). "],
["simulate-rate-changes-this-is-needed-to.html", "5.1 Simulate Rate Changes - this is needed to", " 5.1 Simulate Rate Changes - this is needed to The goal is to reasonably approximate rate changes for a book business. "],
["simulate-written-premium-transactions.html", "5.2 Simulate Written Premium Transactions", " 5.2 Simulate Written Premium Transactions Does it matter if we use earnings or written premiuums "],
["research.html", "Section 6 Research", " Section 6 Research Further research needs to be done in order to justify this work with the department of insurance. In other words, what guidance can be provided so that departments of insurance can accept this method. Here we resarch possible pitfalls with this method and the limitations. Also, can we use these models for production purposes. "],
["number-of-price-points-and-needed-layers.html", "6.1 Number of Price Points and Needed Layers", " 6.1 Number of Price Points and Needed Layers "],
["testing-missing-features.html", "6.2 Testing Missing Features", " 6.2 Testing Missing Features "],
["is-it-right.html", "6.3 Is it right?", " 6.3 Is it right? Depends on the data. If the insurance data is wrong it produce correct onleveled premiums for the actuary but it will not point out production problems in rating. A few rating examples can datermine. "],
["method.html", "6.4 Method", " 6.4 Method How am I approaching data creation. 6.4.1 Does it matter if we do not correlation among cohorts? 6.4.1.1 Without correlation 6.4.1.2 With correlation "],
["more.html", "6.5 More", " 6.5 More 6.5.1 Rating Expert Feedback Feedback is very important to any model. Individual rating examples provided by actuarial or other staff 6.5.2 Revisiting Rating Examples with Deeper Nets With all the background up to this point we introduce depper networks to represenet non-linear rating example. "],
["copybooks-and-cobol.html", "6.6 Copybooks and COBOL", " 6.6 Copybooks and COBOL A COBOL copybook is a selection of code that defines data structures. If a particular data structure is used in many programs, then instead of writing the same data structure again, we can use copybooks. Many insurance companies utilize an old IBM mainframe and likely use copybooks to describe rating data. The great thing about copybooks is that it is organized data and it uses codes (hexidecimal) to reduce space. Chuck, can you show a copybook format and make sure it works with NNs. "]
]
