[
["index.html", "Learning Extension of Exposures with Neural Networks in R Abstract", " Learning Extension of Exposures with Neural Networks in R Charles Lindberg, FCAS 2020-06-06 Abstract Motivation. Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. Over the years a company may have licensed many writing companies each with their “code” or rating algorithm across multiple systems with varying versions of code. This makes it difficult to rate policies in batch. IT and actuaries do not have the ability to do this. Replatforming is becoming more and more of an issue. Method. It is standard practice to onlevel policy premiums to current (or possibly prior) rate levels and it is best practice to use some adaptation of extension of exposures. A theory is proposed when data is missing. Variability should not be an issue- rating algorithms are a closed system so there should be very little variability or not at all. Results. As long as premiums are rated correctly. The error term can be used to assess risk. Error in premiums can be directly compared to actuarial indications. Conclusions. Using a neural network: A PMML can be created, we can use many data scources. How does it handle mutiple writing companies. Once state versus another. State nuances? Availability. The data source if the most important aspect. The data needs to be from the direct source of the data. A downstream source will lose information. Keywords. Neural Networks; Extension of Exposures; Parallelogram Method; Onleveling; "],
["about-the-author.html", "About the Author", " About the Author Chuck Lindberg is a Fellow of the Casualty Actuarial Society who studied mathematics at Utah Valley University. He has ten years of experience in US Property &amp; Casualty insurance with applications in predictice modeling and data science. Lindberg is a strong advocate for modern actuarial best practices to include reproducibility and make it simpler for actuaries to research, analyze, and interpret rather than crunch figures. "],
["introduction.html", "Section 1 Introduction", " Section 1 Introduction Insurance rating algorithms are becoming more complex as companies replatform to new technologies, find more ways to segment costs, and identify and adapt to new data sources. It is becoming increasingly difficult for actuaries and other insurance professionals to obtain accurate onleveled premium at the exposure level. This may happen for several reasons: The company did not invest in re-rating technologies, capturing critical information, or detailed exposure data. The demands on the actuary to provide policy or exposure level data is increasing. Rating systems are on outdated IT platforms, many from the innovations in the 1970. These systems work well but are not well understood in many cases. Business requirements were not robust enough or missed details when developing rating systems. There is a need for a faster more accurate approach to approximate onlevel premiums. Traditional approaches are time-consuming and are often left with problems that never get resolved1. Bring policy premiums to current rate level for actuarial and other financial analysis. Verifying policy rating systems are compliant with intended rates. Data scienstists and research professionals need onleveled premiums for some models and the traditional techniques (such as pallelogram method) do not onlevel premium accurately enough at a lower grain. Incorrect premiums can add skew and mis-align results. (Chuck, can you show this somehow?) Product/portfolio managers want to view their book of business in finer more segmented detail. Accurate premiums can eliminate some bias or reduce the number of assumptions when facing a business decision. A rating system is often used to simulate rate changes and view impacts to policyholders. These impacts, sometimes called dislocation, is often shared with US State Departments in the form of a histogram when a company is making pricing changes. Over the years a company may have licensed many writing companies each with their “code” or rating algorithm across multiple systems with varying versions of code. This makes it difficult even more difficult to rate policies in batch. If IT and actuaries do not have the ability to do this then assumptions need to be made and particular details need to be addressed directly. The traditional approach when designing an extension of exposures algorithm is to code every rating step. A batch rater would require data preprocessing as well— this is often difficult if the data is not mapped to rating elements directly.↩ "],
["extension-of-exposures.html", "1.1 Extension of Exposures", " 1.1 Extension of Exposures Extension of exposures (EoE) is the method in which every policy is rated according to some “rate book” version which typically includes these components: Set of rate tables. Rating steps / Rating formula / Rating Algorithm. Policy attribute data / rating data. This method restates the historical premium to the amount that would be charged under the rates set forth by the rate book. In theory, this could be at any rate level as prescribed by the rate book version. Extension of exposures has the advantage of being the most accurate current rate level method, however, the major disadvantage it has is the amount of time and effort it takes to maintain. In the past, extension of exposures was practically impossible due to the significant number of calculations required to rerate each policy. Given the tremendous increase in computing power, the only remaining hurdle is associated with gathering the required data. To adjust premium to the current rate level using the extension of exposures technique, the practitioner needs to know the applicable rating characteristics for every policy in the historical period. Often companies do not have that information readily available. The leennr approach is meant to address the disadvantages of the extension of exposures method: rating algorithm maintenance and data availability. Because rating algorithms are so unique there is no single approach that will work in all cases. With luck this book’s guidance will produce faster more granular on-leveled premiums. "],
["why-neural-nets.html", "1.2 Why neural nets?", " 1.2 Why neural nets? Neural nets have been said to compute any function within certain approximation (http://neuralnetworksanddeeplearning.com/chap4.html). There is a tradeoff between accuracy and computational time; the deeper complexity of the neural net leads to more accuracy but longer learning time. These approximations are good enough for most actuarial applications as we’ll see later. Premiums are a function of \\(x\\) features, a continuous value greater than 0 but certainly not large. Premiums come from a closed system with many but only finite possible outcomes. Neural networks approximate functions with such properties. Not all outcomes are not evenly distributed which is often referred to as mix of business. This may pose challenges when learning small segments of data which is a limitation of any model. The traditional approach consists of tracking down the exact value of every feature and matching the levels exactly to the rate tables. When a change is made to the algorithm, a rating variable, or table structure, this must be manually programmed again by actuaries. It can be difficult to keep up with. The leennr approach uses neural networks to speed up this process by eliminating data to rate table mappings. Rather than matching values to rate tables, the values can be treated like features in a model. Each feature may have some bearing on the final result which is premium. In theory, the parameters learned will be close to the rating factors in the rate tables, but ultimately, if the algorithm approximates premium well enough for the purposes of the problem that is all that matters. We are going to use neural networks to approximate the “premium function”. Getting within a specified tolerance is our goal. The downside to this approach, which we will explore later, is the complete reliance on data. A quoting engine has different data than a policy writing engine as an example. The premium data generated and collected needs to be understood by the practitionor above all else when using leennr. For example, by its definition if the premium data has errors the neural network would target the “incorrect premiums”, and learn a system that contains those errors. "],
["ratebooks.html", "Section 2 Rate Books and Rate Calculation Examples", " Section 2 Rate Books and Rate Calculation Examples In this section we provide rate books and their respective rating examples that will be used throughout this book. Different lines of business were chosen to show differences each with a level of difficulty associated with it. TABLE 2.1: Rate Calculation Example Summary Line of Business Difficulty Nuance Homeowners Easy Additive components; e.g. Expense Fee "],
["homeowners.html", "2.1 Homeowners", " 2.1 Homeowners Homeowners insurance covers damage to the property, contents, and outstanding structures, as well as loss of use, liability and medical coverage. The perils covered and amount of insurance provided is detailed in the policy contract. This rate book is based on all-perils combined and all coverages combined for simplicity. 2.1.1 Homeowners - Rating Algorithm The rating algorithm is defined as follows. ## # A tibble: 12 x 1 ## expression ## &lt;chr&gt; ## 1 + (All-Peril Base Rate) ## 2 x (AOI Relativity) ## 3 x (Territory Relativity) ## 4 x (Protection Class / Construction Type Relativity) ## 5 x (Underwriting Tier Relativity) ## 6 x (Deductible Credit) ## 7 x [1 - (New Home Discount)] ## 8 x [1 - (Claims-Free Discount)] ## 9 x [1 - (Multi-Policy Discount)] ## 10 + (Increased Jewelry Coverage Rate) ## 11 + (Increased Liability/Medical Coverage Rate) ## 12 + (Policy Fee) The rate tables associated with each rating step can be mapped as follows. variable rate_table All-Peril Base Rate tbl_base AOI Relativity tbl_aoi Territory Relativity tbl_territory Protection Class / Construction Type Relativity tbl_protection_class Underwriting Tier Relativity tbl_uw_tier Deductible Credit tbl_deductible New Home Discount tbl_credits Claims-Free Discount tbl_credits Multi-Policy Discount tbl_credits Increased Jewelry Coverage Rate tbl_jewelry Increased Liability/Medical Coverage Rate tbl_increased_liability Policy Fee tbl_expense_fee 2.1.2 Homeowners - Rate Tables The rate tables are defined as follows. Note the amount of insurance (aoi) table does not typically have a max. Programming each additional factors (extrapolation) and interpolation can be cumbersome. At this time, we use rounded aoi and use a direct lookup rather than interpolation which is often used in practice. # For display purposes we&#39;re going to order based on table length and split into groups of 3. ho_rate_tables %&gt;% list(.$tbl_base, .$tbl_territory, .$tbl_deductible) %&gt;% kable %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) peril value all_perils 500 aoi value 80 0.56 95 0.63 110 0.69 125 0.75 140 0.81 155 0.86 170 0.91 185 0.96 200 1.00 215 1.04 230 1.08 245 1.12 260 1.16 275 1.20 290 1.24 305 1.28 320 1.32 335 1.36 350 1.39 365 1.42 380 1.45 395 1.48 410 1.51 425 1.54 440 1.57 455 1.60 470 1.63 485 1.66 500 1.69 territory value 1 0.80 2 0.90 3 1.00 4 1.10 5 1.15 protection_class construction_type value 1 frame 1.00 2 frame 1.00 3 frame 1.00 4 frame 1.00 5 frame 1.05 6 frame 1.10 7 frame 1.15 8 frame 1.25 9 frame 2.10 10 frame 2.30 1 masonry 0.90 2 masonry 0.90 3 masonry 0.90 4 masonry 0.90 5 masonry 1.00 6 masonry 1.05 7 masonry 1.10 8 masonry 1.15 9 masonry 1.75 10 masonry 1.90 uw_tier value A 0.82 B 0.94 C 1.06 D 1.18 E 1.30 F 1.42 G 1.54 H 1.66 I 1.78 J 1.90 deductible value 250 1.00 500 0.95 1000 0.85 5000 0.70 credit value new home 0.20 claims free 0.10 multipolicy 0.08 jewelry_limit value 2500 0 5000 35 10000 60 limit_liability_medpay value $100,000/$500 0 $300,000/$1,000 25 $500,000/$2,500 45 form value HO3 50 peril value all_perils 500 territory value 1 0.80 2 0.90 3 1.00 4 1.10 5 1.15 deductible value 250 1.00 500 0.95 1000 0.85 5000 0.70 ho_rate_tables %&gt;% list(.$tbl_aoi, .$tbl_construction_type, .$tbl_uw_tier) %&gt;% kable %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) peril value all_perils 500 aoi value 80 0.56 95 0.63 110 0.69 125 0.75 140 0.81 155 0.86 170 0.91 185 0.96 200 1.00 215 1.04 230 1.08 245 1.12 260 1.16 275 1.20 290 1.24 305 1.28 320 1.32 335 1.36 350 1.39 365 1.42 380 1.45 395 1.48 410 1.51 425 1.54 440 1.57 455 1.60 470 1.63 485 1.66 500 1.69 territory value 1 0.80 2 0.90 3 1.00 4 1.10 5 1.15 protection_class construction_type value 1 frame 1.00 2 frame 1.00 3 frame 1.00 4 frame 1.00 5 frame 1.05 6 frame 1.10 7 frame 1.15 8 frame 1.25 9 frame 2.10 10 frame 2.30 1 masonry 0.90 2 masonry 0.90 3 masonry 0.90 4 masonry 0.90 5 masonry 1.00 6 masonry 1.05 7 masonry 1.10 8 masonry 1.15 9 masonry 1.75 10 masonry 1.90 uw_tier value A 0.82 B 0.94 C 1.06 D 1.18 E 1.30 F 1.42 G 1.54 H 1.66 I 1.78 J 1.90 deductible value 250 1.00 500 0.95 1000 0.85 5000 0.70 credit value new home 0.20 claims free 0.10 multipolicy 0.08 jewelry_limit value 2500 0 5000 35 10000 60 limit_liability_medpay value $100,000/$500 0 $300,000/$1,000 25 $500,000/$2,500 45 form value HO3 50 aoi value 80 0.56 95 0.63 110 0.69 125 0.75 140 0.81 155 0.86 170 0.91 185 0.96 200 1.00 215 1.04 230 1.08 245 1.12 260 1.16 275 1.20 290 1.24 305 1.28 320 1.32 335 1.36 350 1.39 365 1.42 380 1.45 395 1.48 410 1.51 425 1.54 440 1.57 455 1.60 470 1.63 485 1.66 500 1.69 uw_tier value A 0.82 B 0.94 C 1.06 D 1.18 E 1.30 F 1.42 G 1.54 H 1.66 I 1.78 J 1.90 ho_rate_tables %&gt;% list(.$tbl_credits, .$tbl_jewelry, .$tbl_increased_liability) %&gt;% kable %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) peril value all_perils 500 aoi value 80 0.56 95 0.63 110 0.69 125 0.75 140 0.81 155 0.86 170 0.91 185 0.96 200 1.00 215 1.04 230 1.08 245 1.12 260 1.16 275 1.20 290 1.24 305 1.28 320 1.32 335 1.36 350 1.39 365 1.42 380 1.45 395 1.48 410 1.51 425 1.54 440 1.57 455 1.60 470 1.63 485 1.66 500 1.69 territory value 1 0.80 2 0.90 3 1.00 4 1.10 5 1.15 protection_class construction_type value 1 frame 1.00 2 frame 1.00 3 frame 1.00 4 frame 1.00 5 frame 1.05 6 frame 1.10 7 frame 1.15 8 frame 1.25 9 frame 2.10 10 frame 2.30 1 masonry 0.90 2 masonry 0.90 3 masonry 0.90 4 masonry 0.90 5 masonry 1.00 6 masonry 1.05 7 masonry 1.10 8 masonry 1.15 9 masonry 1.75 10 masonry 1.90 uw_tier value A 0.82 B 0.94 C 1.06 D 1.18 E 1.30 F 1.42 G 1.54 H 1.66 I 1.78 J 1.90 deductible value 250 1.00 500 0.95 1000 0.85 5000 0.70 credit value new home 0.20 claims free 0.10 multipolicy 0.08 jewelry_limit value 2500 0 5000 35 10000 60 limit_liability_medpay value $100,000/$500 0 $300,000/$1,000 25 $500,000/$2,500 45 form value HO3 50 credit value new home 0.20 claims free 0.10 multipolicy 0.08 jewelry_limit value 2500 0 5000 35 10000 60 limit_liability_medpay value $100,000/$500 0 $300,000/$1,000 25 $500,000/$2,500 45 ho_rate_tables$tbl_expense_fee %&gt;% kable %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) form value HO3 50 "],
["rating-calculation-examples.html", "2.2 Rating Calculation Examples", " 2.2 Rating Calculation Examples In this section we provide a rating example for each rate book presented as a full-term quoted premium. Th 2.2.1 Homeowners The policy has the following risk characteristics: Amount of insurance = $275,000. The insured lives in Territory 2. The home is frame construction located in Fire Protection Class 8. Based on the insured’s credit score, tenure with the company, and prior loss history, the policy has been placed in Underwriting Tier C. The insured opts for a $1,000 deductible. The home falls under the definition of a new home as defined by the rating rules. The insured is eligible for the five-year claims-free discount. There is no corresponding auto or excess liability policy. The policyholder opts to increase coverage for jewelry to $5,000 and to increase liability/medical coverage limits to $300,000/$1,000. The calculation is expressed as follows. \\(500 \\times 1.20 \\times 0.9 \\times 1.25 \\times 1.06 \\times 0.85 \\times (1 - 0.2) \\times (1 - 0.1) \\times (1 - 0) + 35 + 50 = 522.9\\) The final result is rounded to the nearest dollar so the final premium for this quote is $523. "],
["rate-book-complexity.html", "2.3 Rate Book Complexity", " 2.3 Rate Book Complexity One way to measure the complexity of a rate book is by how many price points are possible. In practice, this doesn’t always mean every scenario will occur or be possible. For example, age of insured is a widely used rating variable with high predictive power; the rate table may contain values for 120 years olds, but in reality there are no 120 year olds who need insurance. That being said, the number of possible price points may also measure flexibility of the rating system. Complexity in this case is referring only to the number of possibilities, not a measure of sophistication. The number of possible price points for each rate book is as follows: tribble( ~rate_book, ~complexity, &quot;Homeowners&quot;, ho_rate_tables %&gt;% sapply(nrow) %&gt;% prod ) %&gt;% kableExtra::kable(caption = &quot;Rate Book Complexity Score&quot;) %&gt;% kableExtra::kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = FALSE) TABLE 2.2: Rate Book Complexity Score rate_book complexity Homeowners 3132000 "],
["premium-data.html", "Section 3 Generating Premium Data", " Section 3 Generating Premium Data To get get started we need data that contains rating characteristics. These rating characteristics (also called features) will be passed through a rate book to generate premiums. In this section, we create datasets using the rating examples we saw in the last section. Effectively, we can generate a book of business with any particular mix we like. This will come in handy later when test assumptions and prove the merits of this approach. "],
["uniformly-distributed-independent-mix-with-exact-features.html", "3.1 Uniformly Distributed, Independent Mix, with Exact Features", " 3.1 Uniformly Distributed, Independent Mix, with Exact Features Let’s define what is meant by uniformly distributed, independent mix, with exact features. Uniformly Distributed: each level has the same chance of being drawn as any other level. Independent Mix: rating variables are not correlated with one another whatsoever. Exact Features: features are drawn from the rate tables so the exact value can be found in the rate table. This is the simplest dataset to create. We randomly sample a row from each rate table and bind the result all together to create the exposure record. This code generates feature data we can use to rate with. # Number of records we want to generate. n_draw &lt;- 10000 ho_rate_tables &lt;- read_rds(&quot;ho_rate_tables.Rds&quot;) #Set seed to get consistent results each draw. set.seed(5555) # For each table, draw n_draw random numbers. ho_uniform_iid &lt;- ho_rate_tables %&gt;% lapply( function(x) { nrow(x) %&gt;% sample(n_draw, replace = TRUE) %&gt;% lapply(function(y) x[y, -ncol(x)]) %&gt;% bind_rows}) %&gt;% bind_cols %&gt;% cbind(policy_number = 1:nrow(.) %&gt;% str_pad(nchar(n_draw), pad = &quot;0&quot;), .) ho_uniform_iid %&gt;% as_tibble ## # A tibble: 10,000 x 12 ## policy_number peril aoi territory protection_class ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00001 all_~ 140 3 10 ## 2 00002 all_~ 125 1 7 ## 3 00003 all_~ 80 5 6 ## 4 00004 all_~ 290 4 3 ## 5 00005 all_~ 290 4 1 ## 6 00006 all_~ 170 3 4 ## 7 00007 all_~ 230 2 2 ## 8 00008 all_~ 290 1 8 ## 9 00009 all_~ 140 1 6 ## 10 00010 all_~ 95 3 9 ## # ... with 9,990 more rows, and 7 more variables: ## # construction_type &lt;chr&gt;, uw_tier &lt;chr&gt;, ## # deductible &lt;dbl&gt;, credit &lt;chr&gt;, ## # jewelry_limit &lt;dbl&gt;, limit_liability_medpay &lt;chr&gt;, ## # form &lt;chr&gt; Next, retrieve the rating factors and apply them according to the rating algorithm steps to generate premium as follows. One thing to mention here. In reality a policy can have multiple credits but this code only randomly draws one from the credit table the way it is setup. This doesn’t take away from our experiment. # Need to use the formula we already laid out above somehow. This is just to keep going. ho_uniform_iid_rating &lt;- ho_rate_tables %&gt;% lapply(function(x) (x %&gt;% right_join(ho_uniform_iid))$value) %&gt;% bind_rows %&gt;% mutate( premium = tbl_base * tbl_aoi * tbl_territory * tbl_protection_class * tbl_uw_tier * tbl_deductible * (1 - tbl_credits) + tbl_jewelry + tbl_increased_liability + tbl_expense_fee, premium = premium %&gt;% round ) ## Joining, by = &quot;peril&quot; ## Joining, by = &quot;aoi&quot; ## Joining, by = &quot;territory&quot; ## Joining, by = c(&quot;protection_class&quot;, &quot;construction_type&quot;) ## Joining, by = &quot;uw_tier&quot; ## Joining, by = &quot;deductible&quot; ## Joining, by = &quot;credit&quot; ## Joining, by = &quot;jewelry_limit&quot; ## Joining, by = &quot;limit_liability_medpay&quot; ## Joining, by = &quot;form&quot; # Inpute premium data onto rating data ho_uniform_iid &lt;- ho_uniform_iid %&gt;% cbind(premium = ho_uniform_iid_rating$premium) %&gt;% mutate_if(is.character, factor) Let’s take a look at the distribution of premiums. ho_uniform_iid %&gt;% ggplot + geom_histogram(aes(x = premium), binwidth = 50, color = &quot;darkorange3&quot;, fill = &quot;white&quot;) + xlab(&quot;Premium Bin&quot;) + ylab(&quot;Number of Training Examples&quot;) + theme_bw() This is great result. An insurance rating plan uses ratings factors typically generated using loss cost models. Even though we randomly generated a feature set there is a clear transfomation that occurs just by using rating factors selected from generlized linear model. "],
["not-uniformly-distributed-independent-mix-of-business.html", "3.2 Not Uniformly Distributed, Independent Mix of Business", " 3.2 Not Uniformly Distributed, Independent Mix of Business "],
["not-uniformly-distributed-dependent-mix-of-business.html", "3.3 Not Uniformly Distributed, Dependent Mix of Business", " 3.3 Not Uniformly Distributed, Dependent Mix of Business "],
["generating-historical-book-of-business.html", "3.4 Generating Historical Book of Business", " 3.4 Generating Historical Book of Business To this point we have generated datasets that mirror what we’ll call a quoting environment rather than a policy writing environment. A quoting environment is only generating full-term premiums based on the characteristics provided. The policy writing system includes endorsements which can generate different premiums over time if the endorsement is “premium bearing”. The main reason for the differences is simple: time. Quoting is at one point in time with rating features fixed, but a policy can change during it’s term. In other words, the rating features can change over the life of the policy contract for various reasons— for example, if the insured changes their policy deductible mid-term. In this section we develop a mock historical book of business. To keep it simple, we are not going to use new business and retention assumptions. In other words, we will start with one set of simulated policy data and age it to simulate five years worth of premium data with rate changes introduced. Policies falling off the book and new business coming on will shift the mix but the 5[proof of concept] does not require such detail at this time. In addition, again for simplicity, we are going to build in a rate change for each year— so since we are generating five years of data we will include four rate changes. 3.4.1 Uniformly Distributed ho_uniform_iid$policy_year &lt;- Sys.Date() %&gt;% lubridate::year() - 5 bob_ho_uniform_iid &lt;- ho_uniform_iid %&gt;% mutate( policy_year = policy_year + 1, aoi = (1.05 * aoi) %&gt;% round ) # lubridate::ceiling_date(unit = &quot;years&quot;) - 1 # lubridate::floor_date(unit = &quot;years&quot;) # lubridate::year() # An example of sampling dates # sample(seq(as.Date(&#39;1999/01/01&#39;), as.Date(&#39;2000/01/01&#39;), by=&quot;day&quot;), nrow(ho_uniform_iid), replace = TRUE) ho_uniform_iid %&gt;% write_rds(&quot;ho_uniform_iid.Rds&quot;) 3.4.2 Rate Book Changes "],
["extreme-sample-bias.html", "3.5 Extreme Sample Bias", " 3.5 Extreme Sample Bias What if 90% of the data was male rather than female. "],
["first-time-fit.html", "Section 4 Learning Extension of Exposures - Fitting a Dataset ", " Section 4 Learning Extension of Exposures - Fitting a Dataset "],
["data-preparation-and-required-packages.html", "4.1 Data Preparation and Required Packages", " 4.1 Data Preparation and Required Packages Data has to be on some exposure basis with each record (row) containing the premium target. Each row is a training example. See @ref(premium_data)[premium data] for more detail. Data must also be one hot key encoded before processing as well. The keras package offers a great way to quickly fit neural nets for research. Refer to the keras package for more detail and most everything you need to know about modeling with keras. The Keras site is a great place to start. I’m using the GPU setup of tensorflow— it takes longer to setup but it fits the model much faster. Note we did not scale any of the features or the output. This is not best practice. Because we’re not using many features, the dataset is pretty small (e.g. 10000 rows) and we know the range of premiums is limited from 170 to 3398 it won’t take extra computational time.sav "],
["model-setup.html", "4.2 Model Setup", " 4.2 Model Setup We keep the model pretty simple for now. The ReLu activation is used to keep things greater than zero and not limited by a maximum. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 64, activation = &#39;relu&#39;, input_shape = c(x_train %&gt;% ncol)) %&gt;% layer_dense(units = 32, activation = &#39;relu&#39;) %&gt;% layer_dense(units = 1 , activation = &#39;relu&#39;) model %&gt;% compile( loss = &#39;mse&#39;, optimizer = optimizer_rmsprop() ) "],
["first-time-leennr.html", "4.3 First Time leennR", " 4.3 First Time leennR Fitting the model is quite simple. This model will is trained for 1000 epochs. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 1000, batch_size = 1000, validation_split = 0.2, ) The model summary, the fitting history, and the mean squared error are as follows. ## Model: &quot;sequential&quot; ## _______________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================= ## dense (Dense) (None, 64) 768 ## _______________________________________________________ ## dense_1 (Dense) (None, 32) 2080 ## _______________________________________________________ ## dense_2 (Dense) (None, 1) 33 ## ======================================================= ## Total params: 2,881 ## Trainable params: 2,881 ## Non-trainable params: 0 ## _______________________________________________________ ## [1] &quot;Mean absolute error on test set: $26.73&quot; We can plot how far off the predictions are with histograms. The results show we have some additional research to perform if we want to refine the accuracy further. We’ll explore this further in the next section. # predict %&gt;% # ggplot + # geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) + # xlab(&quot;Premium&quot;) + # ylab(&quot;Absolute Error as % of Premium&quot;) predict %&gt;% ggplot + geom_hex(aes(x = premium, y = premium - y_hat)) + xlab(&quot;Premium&quot;) + ylab(&quot;Premium - Fitted Value&quot;) # predict_test %&gt;% # ggplot + # geom_jitter(aes(x = premium, y = abs(y_hat - premium)/premium)) + # xlab(&quot;Premium&quot;) + # ylab(&quot;Absolute Error as % of Premium&quot;) predict_test %&gt;% ggplot + geom_hex(aes(x = premium, y = premium - y_hat)) + xlab(&quot;Premium&quot;) + ylab(&quot;Premium - Fitted Value&quot;) predict %&gt;% ggplot + geom_histogram(aes(x = premium, color = &quot;premium&quot;), binwidth = 50, color = &quot;darkorange3&quot;, fill = &quot;white&quot;) + geom_histogram(aes(x = y_hat), binwidth = 50, color = &quot;darkblue&quot;, fill = &quot;white&quot;) + xlab(&quot;Premium Bin&quot;) + ylab(&quot;Number of Training Examples&quot;) + theme_bw() "],
["poc.html", "Section 5 leennr Proof of Concept", " Section 5 leennr Proof of Concept There are two different parts to the leennr proof of concept. First, can fitting neural networks approximate extension of exposures within a reasonable tolerance and second, the more difficult can we use the neural nets as a valid rating algorithm, i.e. can we file it? In section @ref(first_time_fit) we showed how to fit a neural network to premium data. The next step is to expand on this idea and determine if the method does reasonably well modeling a rate structure to re-rate polices to a specific rate book (i.e. rate level). "],
["simulate-rate-changes-this-is-needed-to.html", "5.1 Simulate Rate Changes - this is needed to", " 5.1 Simulate Rate Changes - this is needed to The goal is to reasonably approximate rate changes for a book business. "],
["simulate-written-premium-transactions.html", "5.2 Simulate Written Premium Transactions", " 5.2 Simulate Written Premium Transactions Does it matter if we use earnings or written premiuums "],
["research.html", "Section 6 Research", " Section 6 Research Further research needs to be done in order to justify this work with the department of insurance. In other words, what guidance can be provided so that departments of insurance can accept this method. Here we resarch possible pitfalls with this method and the limitations. Also, can we use these models for production purposes. "],
["number-of-price-points-and-needed-layers.html", "6.1 Number of Price Points and Needed Layers", " 6.1 Number of Price Points and Needed Layers "],
["testing-missing-features.html", "6.2 Testing Missing Features", " 6.2 Testing Missing Features "],
["is-it-right.html", "6.3 Is it right?", " 6.3 Is it right? Depends on the data. If the insurance data is wrong it produce correct onleveled premiums for the actuary but it will not point out production problems in rating. A few rating examples can datermine. "],
["method.html", "6.4 Method", " 6.4 Method How am I approaching data creation. 6.4.1 Does it matter if we do not correlation among cohorts? 6.4.1.1 Without correlation 6.4.1.2 With correlation "],
["more.html", "6.5 More", " 6.5 More 6.5.1 Rating Expert Feedback Feedback is very important to any model. Individual rating examples provided by actuarial or other staff 6.5.2 Revisiting Rating Examples with Deeper Nets With all the background up to this point we introduce depper networks to represenet non-linear rating example. "],
["copybooks-and-cobol.html", "6.6 Copybooks and COBOL", " 6.6 Copybooks and COBOL A COBOL copybook is a selection of code that defines data structures. If a particular data structure is used in many programs, then instead of writing the same data structure again, we can use copybooks. Many insurance companies utilize an old IBM mainframe and likely use copybooks to describe rating data. The great thing about copybooks is that it is organized data and it uses codes (hexidecimal) to reduce space. Chuck, can you show a copybook format and make sure it works with NNs. "],
["generalized-parallelogram-method.html", "A Generalized Parallelogram Method", " A Generalized Parallelogram Method Most methods of ratemaking require the Actuary to adjust premiums to reflect all rate changes that have been implemented. Actuaries use the parallelogram method of finding the portion of the exposures earned from a given rate change. Typically, the assumption is made that exposures are being written at a constant level which make the calculation simpler. In this section, we provide a simple general formula for finding the earned portion of a rate increase for any given policy term, rate effective date, and evaluation period (period during which the premiums are earned). An R function to compute rate change factors is also presented. The ideas presented here are based on a paper from the 1989 CAS proceedings (Bill 1989). References "],
["generalized-parallelogram-terms-and-formulas.html", "A.1 Generalized Parallelogram Terms and Formulas", " A.1 Generalized Parallelogram Terms and Formulas Each term is defined in the table below. Set the beginning of the evaluation period at 0 with each term relative to the beginning of the evaluation period. In addition, make sure to use a common unit a measurement when defining these terms such as a year. The subscript \\(i\\) denotes the index of the rate revision in order from 1 to the number of rate changes \\(n\\) in the evaluation period. tribble( ~Term, ~Definition, &quot;$F$&quot;, &quot;Rate adjustment factor (onlevel factor).&quot;, &quot;$T$&quot;, &quot;Policy term; e.g. 1 year.&quot;, &quot;$E$&quot;, &quot;Length of evaluation period.&quot;, &quot;$D_i$&quot;, &quot;Effective date of a rate revision.&quot;, &quot;$P_i$&quot;, &quot;Portion of earned exposure subject to new rate.&quot;, &quot;r_i&quot;, &quot;Rate change $i$.&quot;, &quot;A_i&quot;, &quot;Time when rate change $i$ is fully earned.&quot; ) %&gt;% kable %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = FALSE) Term Definition \\(F\\) Rate adjustment factor (onlevel factor). \\(T\\) Policy term; e.g. 1 year. \\(E\\) Length of evaluation period. \\(D_i\\) Effective date of a rate revision. \\(P_i\\) Portion of earned exposure subject to new rate. r_i Rate change \\(i\\). A_i Time when rate change \\(i\\) is fully earned. The rate adjustment factor for the experience period is defined as follows: \\[\\begin{equation} F = \\frac{(1+r_1),} \\tag{A.1} \\end{equation}\\] \\(P\\) can be calculated by the following formula: where F is the rate adjustment factor, r is the rate change and P is the portion of the earnqd exposure that was subject to the new rate. "]
]
